{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Machine Learning 1\n",
    "## Predict missing citations in the citation network\n",
    "### Team: PHUNG\n",
    "In this project we are going to predict if there is a citiation between 2 paper using a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data tools\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import time\n",
    "# For textual features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "# For graphical features\n",
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "# Classifiers\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# keras modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Other ML tools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Import the data\n",
    "Our dataset contains 27,770 articles from the period  1992-2003,  each  contains  6  informations  of the paper including: <br>\n",
    "(1)  paper unique ID <br>\n",
    "(2)  publication year (from 1992 to 2003) <br>\n",
    "(3)  title <br>\n",
    "(4)  authors <br>\n",
    "(5)  name of journal <br>\n",
    "(6)  abstract <br>\n",
    "### Load paper information\n",
    "The information is stored in the file: *node_information.csv*. We use library csv to load it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_info = []\n",
    "with open(\"data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(node_info[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "The training data consists of 615,512 rows, each contains a node pair with an associated value 1 if there is a citation between the 2 nodes, 0 if not. <br> \n",
    "Example: <br>\n",
    "9510123 9502114 1 <br> \n",
    "9707075 9604178 1 <br> \n",
    "9312155 9506142 0 <br> \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    full_training_data  = list(reader)\n",
    "full_training_data = [element[0].split(\" \") for element in full_training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(full_training_data))\n",
    "print(full_training_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testing data\n",
    "The testing data consists of 32,648 rows each contains a node pair that we have to predict if there is a citation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    full_testing_data  = list(reader)\n",
    "full_testing_data = [element[0].split(\" \") for element in full_testing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(full_testing_data))\n",
    "print(full_testing_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data into a train set and a test set\n",
    "To asset our model, we need a test set with the true output. Hence we will split the training data into a train set and a test set. <br>\n",
    "Here, we choose 95% of our training data as the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if you want to shuffle the data\n",
    "# NOTE: If you shuffle the data and you want to export the features, \n",
    "#       you have to save the shuffled data as well to save the new order of the data\n",
    "np.random.shuffle(full_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ratio of data to be taken into the train set\n",
    "#NOTE: If you modify this, you JUST have to recalculate the citaion graph\n",
    "#      and its features. DO NOT NEED to recalculate other features as it's the same.\n",
    "to_take = 0.95\n",
    "\n",
    "# Split the training data\n",
    "n_split = int(to_take*len(full_training_data))\n",
    "train_set = full_training_data[:n_split]\n",
    "test_set = full_training_data[n_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Extract the features\n",
    "We are going to exact all the features for our model. <br>\n",
    "### Textual features\n",
    "The textual features contains: <br>\n",
    "(1) Cosine similarity of abstracts <br>\n",
    "(2) Number of overlap words in abstract <br>\n",
    "(3) Cosine similarity of titles <br>\n",
    "(4) Number of overlap words in title <br>\n",
    "(5) Number of overlap words between target's title and source's abstract\n",
    "<br>\n",
    "<br>\n",
    "This's gonna take a little bit long time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_title_out = []\n",
    "cosine_title_test_out = []\n",
    "cosine_abstract_out = []\n",
    "cosine_abstract_test_out = []\n",
    "overlap_title_out = []\n",
    "overlap_title_test_out = []\n",
    "overlap_abstract_out = []\n",
    "overlap_abstract_test_out = []\n",
    "overlap_title_in_abstract_out = []\n",
    "overlap_title_in_abstract_test_out = []\n",
    "\n",
    "# For tokenize the paragraph\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Transform titles and abstracts into TFIDF vectors\n",
    "titles = [element[2] for element in node_info]\n",
    "features_TFIDF_title = vectorizer.fit_transform(titles)\n",
    "\n",
    "abstracts = [element[5] for element in node_info]\n",
    "features_TFIDF_abstract = vectorizer.fit_transform(abstracts)\n",
    "\n",
    "# Start extract the features\n",
    "counter = 0\n",
    "for data in full_training_data[:1000]:\n",
    "    source_id = IDs.index(data[0])\n",
    "    target_id = IDs.index(data[1])\n",
    "    source_info = node_info[source_id]\n",
    "    target_info = node_info[target_id]\n",
    "    \n",
    "    # Overlap title\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    overlap_title_out.append(len(set(source_title).intersection(set(target_title))))\n",
    "    \n",
    "    # Overlap abstract\n",
    "    source_abstract = source_info[5].lower().split(\" \")\n",
    "    source_abstract = [token for token in source_abstract if token not in stpwds]\n",
    "    source_abstract = [stemmer.stem(token) for token in source_abstract]\n",
    "    target_abstract = target_info[5].lower().split(\" \")\n",
    "    target_abstract = [token for token in target_abstract if token not in stpwds]\n",
    "    target_abstract = [stemmer.stem(token) for token in target_abstract]\n",
    "    overlap_abstract_out.append(len(set(source_abstract).intersection(set(target_abstract))))\n",
    "    \n",
    "    # Cosine similarity title\n",
    "    cosine_title_out.append(cosine_similarity(features_TFIDF_title[source_id],features_TFIDF_title[target_id])[0,0])\n",
    "    \n",
    "    # Cosine similarity abstract\n",
    "    cosine_abstract_out.append(cosine_similarity(features_TFIDF_abstract[source_id],features_TFIDF_abstract[target_id])[0,0])\n",
    "    \n",
    "    # Overlap words of target title in source abstract\n",
    "    overlap_title_in_abstract_out.append(len(set(source_abstract).intersection(set(target_title))))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print(counter, \"training examples processsed\")\n",
    "    \n",
    "counter = 0\n",
    "for data in full_testing_data[:1000]:\n",
    "    source_id = IDs.index(data[0])\n",
    "    target_id = IDs.index(data[1])\n",
    "    source_info = node_info[source_id]\n",
    "    target_info = node_info[target_id]\n",
    "    \n",
    "    # Overlap title\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    overlap_title_test_out.append(len(set(source_title).intersection(set(target_title))))\n",
    "    \n",
    "    # Overlap abstract\n",
    "    source_abstract = source_info[5].lower().split(\" \")\n",
    "    source_abstract = [token for token in source_abstract if token not in stpwds]\n",
    "    source_abstract = [stemmer.stem(token) for token in source_abstract]\n",
    "    target_abstract = target_info[5].lower().split(\" \")\n",
    "    target_abstract = [token for token in target_abstract if token not in stpwds]\n",
    "    target_abstract = [stemmer.stem(token) for token in target_abstract]\n",
    "    overlap_abstract_test_out.append(len(set(source_abstract).intersection(set(target_abstract))))\n",
    "    \n",
    "    # Cosine similarity title\n",
    "    cosine_title_test_out.append(cosine_similarity(features_TFIDF_title[source_id],features_TFIDF_title[target_id])[0,0])\n",
    "    \n",
    "    # Cosine similarity abstract\n",
    "    cosine_abstract_test_out.append(cosine_similarity(features_TFIDF_abstract[source_id],features_TFIDF_abstract[target_id])[0,0])\n",
    "    \n",
    "    # Overlap words of target title in source abstract\n",
    "    overlap_title_in_abstract_test_out.append(len(set(source_abstract).intersection(set(target_title))))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print(counter, \"testing examples processsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical features\n",
    "For this part we are going to use the NetworkX library.\n",
    "#### Citation graph's features\n",
    "The expected features are: <br>\n",
    "(6) Number of common neighbourhoods<br>\n",
    "(7) Link-based Jaccard coefficient<br>\n",
    "(8) Adamic-Adar index<br>\n",
    "(9) Preference attachment<br>\n",
    "(10) Difference in betweenness centrality<br>\n",
    "(11) Difference in the number of in-links<br>\n",
    "(12) Number of times target cited<br>\n",
    "(13) Pagerank of source<br>\n",
    "(14) Pagerank of target<br>\n",
    "(15) Minimal distance*<br>\n",
    "(16) Is the same cluster?<br>\n",
    "##### Import the graphs\n",
    "For this part we need 2 graphs, an undirected graph and a directed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.Graph()\n",
    "dG=nx.DiGraph()\n",
    "\n",
    "# Adding nodes\n",
    "for node in node_info:\n",
    "    G.add_node(node[0])\n",
    "    dG.add_node(node[0])\n",
    "    \n",
    "# Adding edges, we only take the egdes data from the train set\n",
    "for data in train_set:\n",
    "    if data[2] == '1':\n",
    "        G.add_edge(data[0],data[1])\n",
    "        dG.add_edge(data[0],data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_nei_out = []\n",
    "common_nei_test_out = []\n",
    "jaccard_coef_out = []\n",
    "jaccard_coef_test_out = []\n",
    "aa_index_out = []\n",
    "aa_index_test_out = []\n",
    "pref_att_out = []\n",
    "pref_att_test_out = []\n",
    "btw_diff_out = []\n",
    "btw_diff_test_out = []\n",
    "in_link_out = []\n",
    "in_link_test_out = []\n",
    "num_cite_out = []\n",
    "num_cite_test_out = []\n",
    "pr_source_out = []\n",
    "pr_source_test_out = []\n",
    "pr_target_out = []\n",
    "pr_target_test_out = []\n",
    "shortest_path_out = []\n",
    "shortest_path_test_out = []\n",
    "same_cluster_out = []\n",
    "same_cluster_test_out = []\n",
    "\n",
    "# Calulate the Betweenness centrality\n",
    "print('Calculating the Betweenness centrality...')\n",
    "# the higher k is, the more exact the results are, can set k to 1000 but it would take 5-10 mins\n",
    "btw = nx.betweenness_centrality(G, k = 100)\n",
    "print('Completed!')\n",
    "\n",
    "# Calculate the Pagerank of the network\n",
    "print('Calculating the Pagerank...')\n",
    "pr = nx.pagerank(G, alpha=0.9)\n",
    "print('Completed!')\n",
    "\n",
    "# Calculate the clustering of the network\n",
    "print('Calculating the cluster...')\n",
    "par =  community_louvain.best_partition(G)\n",
    "print('Completed!')\n",
    "\n",
    "# Start extract the features\n",
    "counter = 0\n",
    "for data in full_training_data:\n",
    "    # Common neighborhoods\n",
    "    common_nei_out.append(len(list(nx.common_neighbors(G, data[0], data[1]))))\n",
    "    \n",
    "    # Jaccard coefficient\n",
    "    jaccard_coef_out.append(list(nx.jaccard_coefficient(G, [(data[0], data[1])]))[0][2])\n",
    "    \n",
    "    # Adamic-Adar index\n",
    "    aa_index_out.append(list(nx.adamic_adar_index(G, [(data[0], data[1])]))[0][2])\n",
    "    \n",
    "    # Preferential attachment\n",
    "    pref_att_out.append(list(nx.preferential_attachment(G, [(data[0], data[1])]))[0][2])\n",
    "    \n",
    "    # Difference in the Betweenness centrality\n",
    "    btw_diff_out.append(btw[data[1]]-btw[data[0]])\n",
    "    \n",
    "    # Difference in the number of in-links\n",
    "    in_link_out.append(len(dG.in_edges(data[1])) - len(dG.in_edges(data[0])))\n",
    "    \n",
    "    # Number of times cited\n",
    "    num_cite_out.append(len(dG.in_edges(data[1])))\n",
    "    \n",
    "    # Pagerank of source\n",
    "    pr_source_out.append(pr[data[0]])\n",
    "    \n",
    "    # Pagerank of target\n",
    "    pr_target_out.append(pr[data[1]])\n",
    "    \n",
    "    # Is in the same cluster\n",
    "    if(par[data[0]]==par[data[1]]):\n",
    "        same_cluster_out.append(1)\n",
    "    else: same_cluster_out.append(0)\n",
    "        \n",
    "    # Minimal distance*\n",
    "    if (data[0], data[1]) in G.edges():\n",
    "        G.remove_edge(data[0], data[1])\n",
    "        try:\n",
    "            shortest_path_out.append(nx.shortest_path_length(G, source=data[0], target= data[1]))\n",
    "        except:\n",
    "            shortest_path_out.append(0)\n",
    "        G.add_edge(data[0], data[1])\n",
    "    else:\n",
    "        try:\n",
    "            shortest_path_out.append(nx.shortest_path_length(G, source=data[0], target= data[1]))\n",
    "        except:\n",
    "            shortest_path_out.append(0)\n",
    "    \n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print(counter, \"training examples processsed\")\n",
    "    \n",
    "counter = 0\n",
    "for data in full_testing_data:\n",
    "    # Common neighborhoods\n",
    "    common_nei_test_out.append(len(list(nx.common_neighbors(G, data[0], data[1]))))\n",
    "    \n",
    "    # Jaccard coefficient\n",
    "    jaccard_coef_test_out.append(list(nx.jaccard_coefficient(G, [(data[0], data[1])]))[0][2])\n",
    "    \n",
    "    # Adamic-Adar index\n",
    "    aa_index_test_out.append(list(nx.adamic_adar_index(G, [(data[0], data[1])]))[0][2])\n",
    "    \n",
    "    # Preferential attachment\n",
    "    pref_att_test_out.append(list(nx.preferential_attachment(G, [(data[0], data[1])]))[0][2])\n",
    "    \n",
    "    # Difference in the Betweenness centrality\n",
    "    btw_diff_test_out.append(btw[data[1]]-btw[data[0]])\n",
    "    \n",
    "    # Difference in the number of in-links\n",
    "    in_link_test_out.append(len(dG.in_edges(data[1])) - len(dG.in_edges(data[0])))\n",
    "    \n",
    "    # Number of times cited\n",
    "    num_cite_test_out.append(len(dG.in_edges(data[1])))\n",
    "    \n",
    "    # Pagerank of source\n",
    "    pr_source_test_out.append(pr[data[0]])\n",
    "    \n",
    "    # Pagerank of target\n",
    "    pr_target_test_out.append(pr[data[1]])\n",
    "    \n",
    "    # Is in the same cluster\n",
    "    if(par[data[0]]==par[data[1]]):\n",
    "        same_cluster_test_out.append(1)\n",
    "    else: same_cluster_test_out.append(0)\n",
    "        \n",
    "    # Minimal distance*\n",
    "    if (data[0], data[1]) in G.edges():\n",
    "        G.remove_edge(data[0], data[1])\n",
    "        try:\n",
    "            shortest_path_test_out.append(nx.shortest_path_length(G, source=data[0], target= data[1]))\n",
    "        except:\n",
    "            shortest_path_test_out.append(0)\n",
    "        G.add_edge(data[0], data[1])\n",
    "    else:\n",
    "        try:\n",
    "            shortest_path_test_out.append(nx.shortest_path_length(G, source=data[0], target= data[1]))\n",
    "        except:\n",
    "            shortest_path_test_out.append(0)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print(counter, \"testing examples processsed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author collaboration graph's features\n",
    "The expected features are: <br>\n",
    "(17) Number of common neighbourhoods <br>\n",
    "(18) Link-based Jaccard coefficient <br>\n",
    "(19) Preference attachment <br>\n",
    "(20) Adamic-Adar index <br>\n",
    "##### Clean and import the authors data\n",
    "We'll try to remove all the parentheses in the author information to give a better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegEx for the parentheses\n",
    "my_regex = \"\\([^()]*\\)\"\n",
    "# RegEx to remove the spaces after . , eg. A. Brown -> A.Brown\n",
    "my_regex_2 = \"\\.\\s+\"\n",
    "\n",
    "author_list = set()\n",
    "for i in range(len(node_info)):\n",
    "    auths = re.sub(my_regex, \"\", node_info[i][3])\n",
    "    auths = auths.split(\"(\")[0]\n",
    "    auths = auths.split(\",\")\n",
    "    corrected_auth = \"\"\n",
    "    for j,auth in enumerate(auths):\n",
    "        auth = auth.lstrip().rstrip()\n",
    "        auth = re.sub(my_regex_2, \".\", auth)\n",
    "        if len(auth) > 1:\n",
    "            if corrected_auth != \"\":\n",
    "                corrected_auth += \",\"\n",
    "            author_list.add(auth)\n",
    "            corrected_auth += auth\n",
    "    node_info[i][3] = corrected_auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the graph\n",
    "For this part we only need the undirected graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_collab=nx.Graph()\n",
    "\n",
    "for auth in author_list:\n",
    "    G_collab.add_node(auth)\n",
    "    \n",
    "for node in node_info:\n",
    "    auths = node[3].split(\",\")\n",
    "    if len(auths) > 1:\n",
    "        for pair in itertools.combinations(auths, 2):\n",
    "            authA = pair[0]\n",
    "            authB = pair[1]\n",
    "            if G_collab.has_edge(authA,authB):\n",
    "                G_collab[authA][authB]['weight'] += 1\n",
    "            else:\n",
    "                G_collab.add_edge(authA,authB, weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_collab.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract the features\n",
    "In this part we consider a paper's neiborhood as the union the neighoborhoods of all the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collab_comm_nei_out = []\n",
    "collab_comm_nei_test_out = []\n",
    "collab_jaccard_coef_out = []\n",
    "collab_jaccard_coef_test_out = []\n",
    "collab_aa_out = []\n",
    "collab_aa_test_out = []\n",
    "collab_pa_out = []\n",
    "collab_pa_test_out = []\n",
    "\n",
    "counter = 0\n",
    "for data in full_training_data:\n",
    "    authorsA = node_info[IDs.index(data[0])][3].split(\",\")\n",
    "    authorsB = node_info[IDs.index(data[1])][3].split(\",\")\n",
    "    \n",
    "    while \"\" in authorsA:\n",
    "        authorsA.remove(\"\")\n",
    "    while \"\" in authorsB:\n",
    "        authorsB.remove(\"\")\n",
    "    \n",
    "    # Calculate the neighbor set of source and target\n",
    "    neiborsA = set()\n",
    "    for author in authorsA:\n",
    "        neiborsA = neiborsA.union(set(list(G_collab.neighbors(author))))\n",
    "    neiborsB = set()\n",
    "    for author in authorsB:\n",
    "        neiborsB = neiborsB.union(set(list(G_collab.neighbors(author))))\n",
    "        \n",
    "    # Number of common neighbors\n",
    "    collab_comm_nei_out.append(len(neiborsA.intersection(neiborsB)))\n",
    "    \n",
    "    # Jaccard coeffient\n",
    "    if len(neiborsA.union(neiborsB)) != 0 :\n",
    "        collab_jaccard_coef_out.append(len(neiborsA.intersection(neiborsB))/float(len(neiborsA.union(neiborsB))))\n",
    "    else:\n",
    "        collab_jaccard_coef_out.append(0)\n",
    "    \n",
    "    # Preferential attachment\n",
    "    collab_pa_out.append(len(neiborsA)*len(neiborsB))\n",
    "    \n",
    "    # Adamic-Adar index, we won't use directly the Network library as we consider all the authors as a node\n",
    "    aa = 0.    \n",
    "    for Z in neiborsA.intersection(authorsB):\n",
    "        if len(list(G_collab.neighbors(Z))) > 1:\n",
    "            aa += 1./np.log(len(list(G_collab.neighbors(Z))))\n",
    "    collab_aa_out.append(aa)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print(counter, \"training examples processsed\")\n",
    "    \n",
    "counter = 0\n",
    "for data in full_testing_data:\n",
    "    authorsA = node_info[IDs.index(data[0])][3].split(\",\")\n",
    "    authorsB = node_info[IDs.index(data[1])][3].split(\",\")\n",
    "    \n",
    "    while \"\" in authorsA:\n",
    "        authorsA.remove(\"\")\n",
    "    while \"\" in authorsB:\n",
    "        authorsB.remove(\"\")\n",
    "    \n",
    "    # Calculate the neighbor set of source and target\n",
    "    neiborsA = set()\n",
    "    for author in authorsA:\n",
    "        neiborsA = neiborsA.union(set(list(G_collab.neighbors(author))))\n",
    "    neiborsB = set()\n",
    "    for author in authorsB:\n",
    "        neiborsB = neiborsB.union(set(list(G_collab.neighbors(author))))\n",
    "        \n",
    "    # Number of common neighbors\n",
    "    collab_comm_nei_test_out.append(len(neiborsA.intersection(neiborsB)))\n",
    "    \n",
    "    # Jaccard coeffient\n",
    "    if len(neiborsA.union(neiborsB)) != 0 :\n",
    "        collab_jaccard_coef_test_out.append(len(neiborsA.intersection(neiborsB))/float(len(neiborsA.union(neiborsB))))\n",
    "    else:\n",
    "        collab_jaccard_coef_test_out.append(0)\n",
    "        \n",
    "    # Preferential attachment\n",
    "    collab_pa_test_out.append(len(neiborsA)*len(neiborsB))\n",
    "    \n",
    "    # Adamic-Adar index\n",
    "    aa = 0.    \n",
    "    for Z in neiborsA.intersection(authorsB):\n",
    "        if len(list(G_collab.neighbors(Z))) > 1:\n",
    "            aa += 1./np.log(len(list(G_collab.neighbors(Z))))\n",
    "    collab_aa_test_out.append(aa)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print(counter, \"testing examples processsed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other features\n",
    "Including: <br>\n",
    "(21) Difference in publication year <br>\n",
    "(22) Journal popularity of target <br>\n",
    "(23) Is the same journal? <br>\n",
    "(24) The number of common authors <br>\n",
    "(25) Is self-cited? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_diff_out = []\n",
    "temp_diff_test_out = []\n",
    "journal_pop_out = []\n",
    "journal_pop_test_out = []\n",
    "same_journ_out = []\n",
    "same_journ_test_out = []\n",
    "comm_auth_out = []\n",
    "comm_auth_test_out = []\n",
    "self_cite_out = []\n",
    "self_cite_test_out = []\n",
    "\n",
    "# For journal popularity, we need to preprocess a little...\n",
    "journal_dict = {}\n",
    "\n",
    "for node in node_info:\n",
    "    if node[4] != \"\":\n",
    "        if node[4] not in journal_dict:\n",
    "            journal_dict[node[4]] = 1\n",
    "        else:\n",
    "            journal_dict[node[4]] += 1            \n",
    "journal_dict[\"\"] = 0.\n",
    "        \n",
    "# Normalize the value\n",
    "factor=1.0/sum(journal_dict.values())\n",
    "for k in journal_dict:\n",
    "      journal_dict[k] = journal_dict[k]*factor\n",
    "\n",
    "counter = 0\n",
    "for data in full_training_data:\n",
    "    source_info = node_info[IDs.index(data[0])]\n",
    "    target_info = node_info[IDs.index(data[1])]\n",
    "    \n",
    "    # Year's difference\n",
    "    temp_diff_out.append(int(source_info[1]) - int(target_info[1]))\n",
    "    \n",
    "    # Journal's popularity of target \n",
    "    journal_pop_out.append(journal_dict[target_info[4]])\n",
    "    \n",
    "    # Is same journal ?\n",
    "    same_journ_out.append(int(source_info[4]==target_info[4]))\n",
    "    \n",
    "    # Number of common authors\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    comm_auth_out.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    \n",
    "    # Is self-cite?\n",
    "    self_cite_out.append(int(len(set(source_auth).intersection(set(target_auth)))>0))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print(counter, \"training examples processsed\")\n",
    "    \n",
    "counter = 0\n",
    "for data in full_testing_data:\n",
    "    source_info = node_info[IDs.index(data[0])]\n",
    "    target_info = node_info[IDs.index(data[1])]\n",
    "    \n",
    "    # Year's difference\n",
    "    temp_diff_test_out.append(int(source_info[1]) - int(target_info[1]))\n",
    "    \n",
    "    # Journal's popularity of target \n",
    "    journal_pop_test_out.append(journal_dict[target_info[4]])\n",
    "    \n",
    "    # Is same journal ?\n",
    "    same_journ_test_out.append(int(source_info[4]==target_info[4]))\n",
    "    \n",
    "    # Number of common authors\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    comm_auth_test_out.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    \n",
    "    # Is self-cite?\n",
    "    self_cite_test_out.append(int(len(set(source_auth).intersection(set(target_auth)))>0))    \n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print(counter, \"testing examples processsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = np.array([cosine_title_out ,\n",
    "                          cosine_abstract_out ,\n",
    "                          overlap_title_out ,\n",
    "                          overlap_abstract_out ,\n",
    "                          overlap_title_in_abstract_out ,\n",
    "                          common_nei_out ,\n",
    "                          jaccard_coef_out ,\n",
    "                          aa_index_out ,\n",
    "                          pref_att_out ,\n",
    "                          btw_diff_out ,\n",
    "                          in_link_out ,\n",
    "                          num_cite_out ,\n",
    "                          pr_source_out ,\n",
    "                          pr_target_out ,\n",
    "                          shortest_path_out ,\n",
    "                          same_cluster_out ,\n",
    "                          collab_comm_nei_out ,\n",
    "                          collab_jaccard_coef_out ,\n",
    "                          collab_aa_out ,\n",
    "                          collab_pa_out ,\n",
    "                          temp_diff_out ,\n",
    "                          journal_pop_out ,\n",
    "                          same_journ_out ,\n",
    "                          comm_auth_out ,\n",
    "                          self_cite_out \n",
    "                         ]).T\n",
    "\n",
    "# Preprocess the features\n",
    "features_data = preprocessing.scale(features_data)\n",
    "print(features_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Running the model\n",
    "We will run our model.\n",
    "### Define train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features_data[:n_split]\n",
    "y_train = [int(data[2]) for data in train_set]\n",
    "X_test = features_data[n_split:]\n",
    "y_test = [int(data[2]) for data in test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn classifiers\n",
    "Run the following codes if want to use sklearn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose the classifier\n",
    "# #classifier = svm.LinearSVC()\n",
    "# #classifier = LogisticRegression()\n",
    "# #classifier = AdaBoostClassifier()\n",
    "# classifier = ExtraTreesClassifier()\n",
    "# #classifier = RandomForestClassifier(n_jobs=1, n_estimators=500, criterion=\"entropy\", max_features=\"log2\", max_depth=10)\n",
    "\n",
    "# # Fit the model\n",
    "# start_time = time.time()\n",
    "# classifier.fit(X_train,y_train)\n",
    "# rtime = time.time() - start_time\n",
    "# print(\"Training completed in \", rtime, \" seconds.\")\n",
    "\n",
    "# # Export the result\n",
    "# y_pred = list(classifier.predict(X_test))\n",
    "\n",
    "# print(\"precision: \",accuracy_score(y_test, y_pred))\n",
    "# print(\"f1 score: \",f1_score(y_test,y_pred , average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras NN model\n",
    "In this case we use a sequenctial model of 3 hidden layers and activation function 'Relu', we use l2 regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "n_features = features_data.shape[1]\n",
    "n_h1 = 30\n",
    "n_h2 = 30\n",
    "n_h3 = 30\n",
    "\n",
    "model = Sequential([\n",
    " Dense(output_dim=n_h1, input_dim=n_features, activation='relu', kernel_regularizer=regularizers.l2(0.00001)),\n",
    " Dense(output_dim=n_h2, input_dim=n_h1, activation='relu', kernel_regularizer=regularizers.l2(0.00001)),\n",
    " Dense(output_dim=n_h3, input_dim=n_h2, activation='relu', kernel_regularizer=regularizers.l2(0.00001)),\n",
    "Dense(output_dim=1, input_dim=n_h3, activation='sigmoid'),\n",
    " ])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [#EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='keras_best_model.hdf5', monitor='val_acc', save_best_only=True)\n",
    "            ]\n",
    "\n",
    "# Fit the model\n",
    "trained_model = model.fit(X_train, y_train, epochs=50, \n",
    "                          callbacks=callbacks, \n",
    "                          batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "model.load_weights(\"keras_best_model.hdf5\")\n",
    "\n",
    "# Export the result\n",
    "y_pred = np.rint(list(model.predict(X_test))).reshape((1,-1))[0].astype(int)\n",
    "\n",
    "print(\"precision: \",accuracy_score(y_test, y_pred))\n",
    "print(\"f1 score: \",f1_score(y_test,y_pred , average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Export the final result on the testing data to csv\n",
    "### Get the features on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_features = np.array([\n",
    "                          cosine_title_test_out ,\n",
    "                          cosine_abstract_test_out ,\n",
    "                          overlap_title_test_out ,\n",
    "                          overlap_abstract_test_out ,\n",
    "                          overlap_title_in_abstract_test_out ,\n",
    "                          common_nei_test_out ,\n",
    "                          jaccard_coef_test_out ,\n",
    "                          aa_index_test_out ,\n",
    "                          pref_att_test_out ,\n",
    "                          btw_diff_test_out ,\n",
    "                          in_link_test_out ,\n",
    "                          num_cite_test_out ,\n",
    "                          pr_source_test_out ,\n",
    "                          pr_target_test_out ,\n",
    "                          shortest_path_test_out ,\n",
    "                          same_cluster_test_out ,\n",
    "                          collab_comm_nei_test_out ,\n",
    "                          collab_jaccard_coef_test_out ,\n",
    "                          collab_aa_test_out ,\n",
    "                          collab_pa_test_out ,\n",
    "                          temp_diff_test_out ,\n",
    "                          journal_pop_test_out ,\n",
    "                          same_journ_test_out ,\n",
    "                          comm_auth_test_out ,\n",
    "                          self_cite_test_out \n",
    "                         ]).T\n",
    "\n",
    "# Preprocess the features\n",
    "testing_data_features = preprocessing.scale(testing_data_features)\n",
    "print(testing_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on the features\n",
    "#### For sklearn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_final = list(classifier.predict(testing_data_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = np.rint(list(model.predict(testing_data_features))).reshape((1,-1))[0].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the final result to the csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = zip(range(len(testing_data_features)), y_pred_final)\n",
    "with open(\"output/my_predictions.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow([\"id\",\"category\"])\n",
    "    for row in final_prediction:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
